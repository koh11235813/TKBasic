{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuTqiuFCrHTM"
   },
   "source": [
    "# 機械学習の数学 5（重回帰分析）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2ARRIk4rHTd"
   },
   "source": [
    "前章の単回帰分析では入力変数 $x$ として部屋の広さを採用し、出力変数 $y$ として家賃を採用しました。  \n",
    "\n",
    "しかし、実際の問題を考える場合、入力変数 $x$ は 1 種類では足りません。家賃を予測するためには部屋の広さ以外にも、駅からの距離・築年数・間取りなど様々な要因が影響する為です。このように、機械学習においては現実に起きている問題を説明する為の適切な入力変数の選定が重要となります。  \n",
    "\n",
    "こういった入力変数を多変数とする場合に重回帰分析を用います。  \n",
    "部屋の広さを $x_{1}$、駅からの距離を $x_{2}$、…、犯罪発生率を $x_{M}$ といった形で表し、$M$ 個の入力変数を扱うことを考えてみましょう。  \n",
    "\n",
    "全体の手順は単回帰分析と同様に進んで行きます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtp5e0-LrHTj"
   },
   "source": [
    "## 本章の構成\n",
    "- モデルを決める\n",
    "- 目的関数を決める\n",
    "- パラメータを最適化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPIVEMntrHTl"
   },
   "source": [
    "## モデルを決める\n",
    "\n",
    "単回帰分析では次のように直線の方程式をモデルとして用いていました。  \n",
    "\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "\n",
    "重回帰分析でも単回帰分析と似た形のモデルを定義します。  \n",
    "また、総和の記号 $∑$ を使って表記することもできます。  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y \n",
    "&= w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{M}x_{M} + b \\\\\n",
    "&= \\sum_{m=1}^{M} w_{m} x_{m} + b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "単回帰分析の際は二次元平面を考え、その平面上に存在するデータに最もよく適合する直線の方程式を考えましたが、今回は $M$ 次元空間に存在するデータに最もよく適合する方程式を見つける事になります。  \n",
    "\n",
    "ここでバイアス $b$ の扱い方を改めて考えます。単回帰分析では、中心化を前処理として施し、バイアス $b$ を省略することで、簡潔に定式化することができました。重回帰分析では、 $M$ 個の重み $w_{1}, w_{2}, \\dots, w_{M}$ と 1 個のバイアス $b$ があり、合わせて $M + 1$ 個のパラメータが存在します。これらのパラメータをうまく定式化することを考えます。  \n",
    "\n",
    "そこで、今回は $x_0 = 1$、$w_0 = b$ とおくことで次のように $b$ を総和の内側の項に含めて、簡潔に表記できるようにします。\n",
    "（ここで、 $\\sum$ 記号の下部が $m=1$ ではなく $m=0$ となっていることに注意してください。）\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y\n",
    "&= w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{M}x_{M} + b \\\\\n",
    "&= w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{M}x_{M} + w_{0}x_{0} \\\\\n",
    "&= w_{0}x_{0} + w_{1}x_{1} + \\cdots + w_{M}x_{M} \\\\\n",
    "&= \\sum_{m=0}^M w_{m} x_{m}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "さらに別の方法として、3 章の線形代数で学んだベクトルの内積で表記するとシンプルに記述できます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y\n",
    "&= w_{0}x_{0} + w_{1}x_{1} + \\cdots + w_{M}x_{M} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_{0} & w_{1} & \\cdots  & w_{M}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "\\vdots \\\\\n",
    "x_{M}\n",
    "\\end{bmatrix} \\\\\n",
    "&= {\\bf w}^{\\rm T}{\\bf x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "このモデルが持つパラメータは前述の通り $M + 1$ 個あり、$M + 1$ 次元のベクトル ${\\bf w}$ で表されています。\n",
    "重回帰分析では、この ${\\bf w}$ のすべての要素について最適な値を求めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY14698ZrHTq"
   },
   "source": [
    "## 目的関数を決める\n",
    "\n",
    "単回帰分析の例と比べると、入力変数は増えましたが、家賃を目標値としている点は変わっていません。  \n",
    "そこで、単回帰分析と同じ目的関数である二乗和誤差を用います。  \n",
    "\n",
    "$$\n",
    "L = (t_1 - y_1)^2 + (t_2 - y_2)^2 + \\cdots + (t_N - y_N)^2\n",
    "$$\n",
    "\n",
    "この二乗和誤差はベクトルの内積を用いて表記すると次の様になります。  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L\n",
    "&= (t_1 - y_1)^2 + (t_2 - y_2)^2 + \\cdots + (t_N - y_N)^2 \\\\\n",
    "&= \\begin{bmatrix}\n",
    "t_1 - y_1 & t_2 - y_2 & \\cdots & t_N - y_N\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "t_1 - y_1 \\\\\n",
    "t_2 - y_2 \\\\\n",
    "\\vdots \\\\\n",
    "t_N - y_N\n",
    "\\end{bmatrix} \\\\\n",
    "&= ({\\bf t} - {\\bf y})^{\\rm T}({\\bf t} - {\\bf y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "405GAb6orHTr"
   },
   "source": [
    "ここで、内積には交換法則が成り立つため、${\\bf w}^{\\rm T}{\\bf x}$ は ${\\bf x}^{\\rm T}{\\bf w}$ と書くこともできます。これを利用して、モデルの方程式 ${\\bf y} = {\\bf w}^{\\rm T}{\\bf x}$ を、以下のように変形します。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf y} =\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "{\\bf x}_1^{\\rm T}{\\bf w} \\\\\n",
    "{\\bf x}_2^{\\rm T}{\\bf w} \\\\\n",
    "\\vdots \\\\\n",
    "{\\bf x}_N^{\\rm T}{\\bf w}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "{\\bf x}_1^{\\rm T} \\\\\n",
    "{\\bf x}_2^{\\rm T} \\\\\n",
    "\\vdots \\\\\n",
    "{\\bf x}_N^{\\rm T}\n",
    "\\end{bmatrix}\n",
    "{\\bf w}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGmt8pKhrHTu"
   },
   "source": [
    "さらに、${\\bf x}_n^{\\rm T} = \\bigl[ x_{n0},\\ x_{n1},\\  x_{n2},\\ \\dots,\\  x_{nM} \\bigr]$ $(n = 1, \\dots, N)$ と展開すると、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf y}\n",
    "&= \\begin{bmatrix}\n",
    "x_{10} & x_{11} & x_{12} & \\cdots & x_{1M} \\\\\n",
    "x_{20} & x_{21} & x_{22} & \\cdots & x_{2M} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N0} & x_{N1} & x_{N2} & \\cdots & x_{NM}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{0} \\\\\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{M}\n",
    "\\end{bmatrix} \\\\\n",
    "&= {\\bf X}{\\bf w}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "と表記できます。\n",
    "ここで、$N \\times M$ 行列 ${\\bf X}$ は、各行が各データを表しており、各列が各入力変数を表しています。各列はそれぞれ入力変数の種類に対応しており、例えば、部屋の広さや、駅からの距離などです。\n",
    "\n",
    "各行が表すデータ点がどのように表されているかを説明するため、具体的な数値例を挙げます。\n",
    "部屋の広さ $= 50{\\rm m}^2$ 、駅からの距離 $= 600 {\\rm m}$ 、犯罪発生率 $= 2\\%$ という 3 つの入力変数を考える場合、$M = 3$ であり、これが $n$ 個目のデータのとき、${\\bf x}_n^{\\rm T}$ は、\n",
    "\n",
    "$$\n",
    "{\\bf x}_n^{\\rm T} =\n",
    "\\begin{bmatrix}\n",
    "1 & 50 & 600 & 0.02\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "となります。先頭に $1$ があるのは、Step 1 で $x_0 = 1$ と定めたためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5g6hCJyrHT6"
   },
   "source": [
    "## パラメータを最適化する\n",
    "\n",
    "それでは、目的関数 $L$ を最小化するモデルのパラメータベクトル ${\\bf w}$ を求めましょう。\n",
    "単回帰分析と同様に、目的関数をパラメータで微分して 0 とおき、${\\bf w}$ について解きます。\n",
    "\n",
    "まずは目的関数に登場している予測値 ${\\bf y}$ を、パラメータ ${\\bf w}$ を用いた表記に置き換えます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L\n",
    "&= ({\\bf t} - {\\bf y})^{\\rm T} ({\\bf t} - {\\bf y}) \\\\\n",
    "&= ({\\bf t} - {\\bf X}{\\bf w})^{\\rm T} ({\\bf t} - {\\bf X}{\\bf w}) \\\\\n",
    "&= \\{ {\\bf t}^{\\rm T} - ({\\bf X}{\\bf w})^{\\rm T} \\} ({\\bf t} - {\\bf X}{\\bf w}) \\\\\n",
    "&= ({\\bf t}^{\\rm T} - {\\bf w}^{\\rm T}{\\bf X}^{\\rm T}) ({\\bf t} - {\\bf X}{\\bf w})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで、転置の公式 $({\\bf A}{\\bf B})^{\\rm T} = {\\bf B}^{\\rm T}{\\bf A}^{\\rm T}$ を用いました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKXSSQosrHT-"
   },
   "source": [
    "さらに分配法則を使って展開すると、\n",
    "\n",
    "$$\n",
    "L\n",
    "= {\\bf t}^{\\rm T}{\\bf t}\n",
    "- {\\bf t}^{\\rm T}{\\bf X}{\\bf w}\n",
    "- {\\bf w}^{\\rm T}{\\bf X}^{\\rm T}{\\bf t}\n",
    "+ {\\bf w}^{\\rm T}{\\bf X}^{\\rm T}{\\bf X}{\\bf w}\n",
    "$$\n",
    "\n",
    "となります。この目的関数に対しパラメータの ${\\bf w}$ についての偏微分を計算します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP0xVTnErHUA"
   },
   "source": [
    "その前に、この式をもう少し整理します。\n",
    "まず、$(1)^{\\rm T} = 1$ のように、スカラーは転置しても変化しません。\n",
    "また、${\\bf w} \\in \\mathbb{R}^{M+1}$、${\\bf X} \\in \\mathbb{R}^{N \\times (M+1)}$ であり、${\\bf X}{\\bf w} \\in \\mathbb{R}^{N}$ となることから、${\\bf t} \\in \\mathbb{R}^{N}$ との間の内積 ${\\bf t}^{\\rm T}{\\bf X}{\\bf w} \\in \\mathbb{R}$ は、スカラーになります。\n",
    "したがって、\n",
    "\n",
    "$$\n",
    "({\\bf t}^{\\rm T}{\\bf X}{\\bf w})^{\\rm T} = {\\bf t}^{\\rm T}{\\bf X}{\\bf w}\n",
    "$$\n",
    "\n",
    "が成り立ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY5veMTlrHUB"
   },
   "source": [
    "さらに、転置の公式 $({\\bf A}{\\bf B}{\\bf C})^{\\rm T} = {\\bf C}^{\\rm T}{\\bf B}^{\\rm T}{\\bf A}^{\\rm T}$ より、\n",
    "\n",
    "$$\n",
    "({\\bf t}^{\\rm T}{\\bf X}{\\bf w})^{\\rm T} = {\\bf w}^{\\rm T} {\\bf X}^{\\rm T} {\\bf t}\n",
    "$$\n",
    "\n",
    "も成り立ちます。以上から、\n",
    "\n",
    "$$({\\bf t}^{T}{\\bf X}{\\bf w})^{T} = {\\bf t}^{T}{\\bf X}{\\bf w} = {\\bf w}^{T} {\\bf X}^{T} {\\bf t}$$\n",
    "\n",
    "が導かれます。目的関数 $\\mathcal{L}$ は、この式を利用して、\n",
    "\n",
    "$$\n",
    "L = {\\bf t}^{\\rm T}{\\bf t} - 2{\\bf t}^{\\rm T}{\\bf X}{\\bf w}+ {\\bf w}^{\\rm T}{\\bf X}^{\\rm T}{\\bf X}{\\bf w}\n",
    "$$\n",
    "\n",
    "と変形できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh0-tA5xrHUC"
   },
   "source": [
    "ここで、${\\bf w}$ に関する偏微分を行いやすくするため、${\\bf w}$ 以外の定数項を一つにまとめます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L\n",
    "&= {\\bf t}^{\\rm T}{\\bf t}\n",
    "- 2{\\bf t}^{\\rm T}{\\bf X}{\\bf w}\n",
    "+ {\\bf w}^{\\rm T}{\\bf X}^{\\rm T}{\\bf X}{\\bf w} \\\\\n",
    "&= {\\bf t}^{\\rm T}{\\bf t}\n",
    "- 2({\\bf X}^{\\rm T}{\\bf t})^{\\rm T} {\\bf w}\n",
    "+ {\\bf w}^{\\rm T}{\\bf X}^{\\rm T}{\\bf X}{\\bf w} \\\\\n",
    "&= c + {\\bf b}^{\\rm T}{\\bf w} + {\\bf w}^{\\rm T}{\\bf A}{\\bf w}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp-tJuGsrHUD"
   },
   "source": [
    "すると、${\\bf w}$ に関する二次形式で表現できました。\n",
    "ここで、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\bf A} &= {\\bf X}^{\\rm T}{\\bf X} \\\\\n",
    "{\\bf b} &= -2 {\\bf X}^{\\rm T}{\\bf t} \\\\\n",
    "c &= {\\bf t}^{\\rm T}{\\bf t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "とおいていることに注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6dgrZVcrHUD"
   },
   "source": [
    "それでは、目的関数を最小にするパラメータ ${\\bf w}$ の求め方を考えます。\n",
    "目的関数はパラメータ ${\\bf w}$ に関して二次関数になっています。\n",
    "まずは、${\\bf w}$ 以外のベクトルや行列に、具体的な数値を当てはめて考えてみましょう。\n",
    "例えば、\n",
    "\n",
    "$$\n",
    "{\\bf w} =\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\ w_2\n",
    "\\end{bmatrix}, \n",
    "{\\bf A} = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix},\n",
    "{\\bf b} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix},\n",
    "c = 1 \n",
    "$$ \n",
    "\n",
    "とおきます。すると、目的関数の値は\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "L\n",
    "&= {\\bf w}^{\\rm T}{\\bf A}{\\bf w} +{\\bf b}^{\\rm T}{\\bf w} + c \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_1 & w_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "w_1 \\\\ \n",
    "w_2 \n",
    "\\end{bmatrix}\n",
    "+\n",
    "1 \\\\ \n",
    "&=\n",
    "\\begin{bmatrix} \n",
    "w_1 & w_2\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "w_1 + 2w_2 \\\\ \n",
    "3w_1 + 4w_2\n",
    "\\end{bmatrix}\n",
    "+ w_1 + 2w_2 + 1 \\\\\n",
    "&= w_1 (w_1 + 2w_2) + w_2 (3w_1 + 4w_2) + w_1 + 2w_2 + 1 \\\\ \n",
    "&= w_1^2 + 5 w_1 w_2 + 4 w_2^2 + w_1 + 2 w_2 + 1 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります。これを $w_1, w_2$ に関して整理すると、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L\n",
    "&= w_1^2 + (5 w_2 + 1) w_1 + (4 w_2^2 + 2 w_2 + 1) \\\\\n",
    "&= 4 w_2^2 + (5 w_1 + 2) w_2 + (w_1^2 + w_1 + 1)\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "となり、$w_1, w_2$ それぞれについて二次関数になっていることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlbA-NGnrHUE"
   },
   "source": [
    "目的関数 $L$ を $w_1$ の二次関数、$w_2$ の二次関数と見たとき、$L$ は、下図のような概形となっています。\n",
    "\n",
    "![01](http://drive.google.com/uc?export=view&id=1dz01fXQXsO0FVkTkTvye3BROe5alf2eS)\n",
    "\n",
    "さらに、各次元が $w_1, w_2, L$ を表す 3 次元空間上においては、 $L$ の概形は下図のようになっています。\n",
    "\n",
    "![02](http://drive.google.com/uc?export=view&id=19B6p25z0-CxeOe9nvLX-lm9jnl_10lBx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQhYFByZrHUF"
   },
   "source": [
    "ここでは 2 つのパラメータ $w_1$ と $w_2$ について図示していますが、目的関数が 任意の $M$ 個の変数 $w_0, w_1, w_2, \\dots, w_M$ によって特徴づけられている場合でも、目的関数がそれぞれのパラメータについて二次形式になっている限り、同様のことが言えます。\n",
    "すなわち、$M + 1$ 個の連立方程式、\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac {\\partial }{\\partial w_0}L = 0 \\\\\n",
    "\\frac {\\partial }{\\partial w_1}L = 0 \\\\\n",
    "\\ \\ \\ \\ \\ \\vdots \\\\\n",
    "\\frac {\\partial }{\\partial w_M}L = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "を解けば良いということになります。\n",
    "これはベクトルによる微分を用いて表記すると、以下のようになります。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "\\frac {\\partial}{\\partial w_0} L \\\\\n",
    "\\frac {\\partial}{\\partial w_1} L \\\\\n",
    "\\vdots  \\\\\n",
    "\\frac {\\partial}{\\partial w_M} L \\\\\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots  \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\Rightarrow \\frac {\\partial}{\\partial {\\bf w}} L\n",
    "&= {\\bf 0} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuYJpGSurHUH"
   },
   "source": [
    "上式を ${\\bf w}$ について解くために、以下のような式変形を行います。\n",
    "式変形の途中で理解できない部分があった場合は、前章の線形代数の部分を読み返してみてください。\n",
    "まずは、左辺について整理を行います。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial {\\bf w}} L\n",
    "&= \\frac{\\partial}{\\partial {\\bf w}} (c + {\\bf b}^{\\rm T}{\\bf w} + {\\bf w}^{\\rm T}{\\bf A}{\\bf w}) \\\\\n",
    "&=\\frac{\\partial}{\\partial {\\bf w}} (c) + \\frac{\\partial}{\\partial {\\bf w}} ({\\bf b}^{\\rm T}{\\bf w}) + \\frac{\\partial}{\\partial {\\bf w}} ({\\bf w}^{\\rm T}{\\bf A}{\\bf w}) \\\\\n",
    "&={\\bf 0} + {\\bf b} + ({\\bf A} + {\\bf A}^{\\rm T}) {\\bf w}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "これを $0$ とおき、${\\bf A}$ 、${\\bf b}$ を展開すると\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-2{\\bf X}^{\\rm T}{\\bf t} + \\{ {\\bf X}^{\\rm T}{\\bf X} + ({\\bf X}^{\\rm T}{\\bf X})^{\\rm T} \\} {\\bf w}\n",
    "&= {\\bf 0} \\\\\n",
    "-2{\\bf X}^{\\rm T}{\\bf t} + 2{\\bf X}^{\\rm T}{\\bf X}{\\bf w}\n",
    "&= {\\bf 0} \\\\\n",
    "{\\bf X}^{\\rm T}{\\bf X}{\\bf w}& = {\\bf X}^{\\rm T}{\\bf t} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のように式変形できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHz8rMH-rHUJ"
   },
   "source": [
    "ここで、${\\bf X}^{\\rm T}{\\bf X}$に**逆行列が存在すると仮定**して、両辺に左側から $({\\bf X}^{\\rm T}{\\bf X})^{-1}$ を掛けると、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "({\\bf X}^{\\rm T}{\\bf X})^{-1} {\\bf X}^{\\rm T}{\\bf X} {\\bf w} &= ({\\bf X}^{\\rm T}{\\bf X})^{-1} {\\bf X}^{\\rm T}{\\bf t} \\\\\n",
    "{\\bf I}{\\bf w} &= ({\\bf X}^{\\rm T}{\\bf X})^{-1} {\\bf X}^{\\rm T}{\\bf t} \\\\\n",
    "{\\bf w} &= ({\\bf X}^{\\rm T}{\\bf X})^{-1}{\\bf X}^{\\rm T}{\\bf t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が導かれます。特に、この最後の式を**正規方程式 (normal equation)** と呼びます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLGWd4xcrHUK"
   },
   "source": [
    "上式は、与えられたデータを並べた行列 ${\\bf X}$ と、各データの目標値を並べたベクトル ${\\bf t}$ から最適なパラメータ ${\\bf w}$ を計算しており、新しい入力変数の値 ${\\bf x} = [x_1, \\dots, x_M]^{\\rm T}$ に対して予測値 $y$ を得るためには、ここで求めたパラメータ ${\\bf w}$を用いて、\n",
    "\n",
    "$$\n",
    "y = {\\bf w}^{\\rm T}{\\bf x}\n",
    "$$\n",
    "\n",
    "のように計算すると良いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUpEBHblrHUL"
   },
   "source": [
    "## 練習問題 本章のまとめ\n",
    "本章で学んだ内容を復習しましょう。問題の答えをそれぞれのセルに記載してください。  \n",
    "（プログラミングを行うのではなく、問題をノート上などで解き、回答を空白のセルに記入してください。）\n",
    "\n",
    "重回帰分析を用いて、パラメータ $w$ を算出してください。数値は下記を用いてください。  \n",
    "（実際にはバイアス $b$ を考慮し計算を行う必要がありますが、この練習問題ではバイアスは考慮せずに計算を行ってください。）\n",
    "\n",
    "|部屋の広さ|駅からの距離|家賃|\n",
    "|:--|:--|:--|\n",
    "|1|2|10|\n",
    "|2|3|14|\n",
    "\n",
    "逆行列の計算は下記を参考にしてください。  \n",
    "\n",
    "下記のように行列 A の逆行列を求める場合は、\n",
    "\n",
    "$$\n",
    "A=\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "下記のような計算となります。\n",
    "\n",
    "$$\n",
    "A^{-1}=\\dfrac{1}{ad-bc}\\begin{pmatrix}d&-b\\\\-c&a\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "実際の値を入れて確認してみます。  \n",
    "$a = 1$ 、$b = 2$ 、$c = 3$ 、$d = 4$ とします。\n",
    "\n",
    "$$\n",
    "A^{-1}=\\dfrac{1}{1 \\times 4 - 2 \\times 3}\\begin{pmatrix}1&2\\\\3&4\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "対角成分（この場合 1 と 4）を入れ替えます。\n",
    "\n",
    "$$\n",
    "A^{-1}=\\dfrac{1}{- 2}\\begin{pmatrix}4&2\\\\3&1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "非対角成分（この場合 2 と 3）を $-1$ 倍します。\n",
    "\n",
    "\n",
    "$$\n",
    "A^{-1}=\\dfrac{1}{- 2}\\begin{pmatrix}4&-2\\\\-3&1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$-$ を行列内に入れます。全ての値の符号が下記のように入れ替わります。\n",
    "\n",
    "$$\n",
    "A^{-1}=\\dfrac{1}{2}\\begin{pmatrix}-4&2\\\\3&-1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "これで逆行列を求めることができました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HDcvGqzKrHUM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-2.  6.]\n"
     ]
    }
   ],
   "source": [
    "# 重回帰分析のパラメータ w の値\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2],\n",
    "              [2, 3]])\n",
    "b = np.array([10, 14])\n",
    "\n",
    "ATA = np.dot(A.T, A)\n",
    "\n",
    "ATA_inv = np.linalg.inv(ATA)\n",
    "\n",
    "ATb = np.dot(A.T, b)\n",
    "\n",
    "w = np.dot(ATA_inv, ATb)\n",
    "\n",
    "\n",
    "print(\"w =\", w)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf w}\n",
    "&={\n",
    "    \\begin{bmatrix}\n",
    "    -2 \\\\\n",
    "    6\n",
    "    \\end{bmatrix}\n",
    "}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thjzjbnIrHUR"
   },
   "source": [
    "### 模範解答\n",
    "\n",
    "$$\n",
    "{\\bf X} = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix} ,\\\n",
    "{\\bf t} = \n",
    "\\begin{bmatrix}\n",
    "10 \\\\\n",
    "14\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "重回帰分析のパラメータ $\\bf{w}$ を求める式はこちらでした。  \n",
    "\n",
    "\n",
    "$$\n",
    "{\\bf w} = ({\\bf X}^{\\rm T}{\\bf X})^{-1}{\\bf X}^{\\rm T}{\\bf t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEDM3uNLrHUU"
   },
   "source": [
    "\n",
    "上記の式に数値を当て込み計算を行います。  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf w} \n",
    "&= \\Biggl(\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 3 \\\\\n",
    "\\end{bmatrix}^{\\rm T}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "\\Biggl)^{-1}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 3 \\\\\n",
    "\\end{bmatrix}^{\\rm T}\n",
    "\\begin{bmatrix}\n",
    "10 \\\\\n",
    "14 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAHoARp4rHUW"
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1g2xjXbw5qYeqdJqcOf3uASvzBQxhlE8u\" width=30%>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TKBasic006_jukaiki.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
